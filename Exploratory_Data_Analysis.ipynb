{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-pHl6H5Yl3t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import spacy\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_reddit_posts = pd.read_csv('preprocessed_reddit_posts.csv')\n",
        "preprocessed_twitter_tweets = pd.read_csv('preprocessed_twitter_tweets.csv')"
      ],
      "metadata": {
        "id": "5G6PFq7SZGNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for Reddit data\n",
        "preprocessed_reddit_posts.describe()"
      ],
      "metadata": {
        "id": "zs8l6YEwZHYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Information about Reddit posts\n",
        "preprocessed_reddit_posts.info()"
      ],
      "metadata": {
        "id": "e82MrwOWZIv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for Twitter tweets\n",
        "preprocessed_twitter_tweets.describe()"
      ],
      "metadata": {
        "id": "F1bVsqFvZKHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Info about Twitter tweets data\n",
        "preprocessed_twitter_tweets.info()"
      ],
      "metadata": {
        "id": "lOR6PCgcZLFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(preprocessed_reddit_posts['score'])\n",
        "plt.title('Distribution of Reddit post scores')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BJh_cq2QZN53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(preprocessed_reddit_posts['score'], preprocessed_reddit_posts['num_comments'])\n",
        "plt.title('Reddit post scores vs. number of comments')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Number of comments')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TwL6B2tJZN1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the title and body columns from the reddit posts df with the tweet_text column from the twitter tweets df\n",
        "merged_df = pd.concat([preprocessed_reddit_posts[\"title\"] + \" \" + preprocessed_reddit_posts[\"body\"], preprocessed_twitter_tweets[\"tweet_text\"]])\n",
        "\n",
        "# Define a regular expression pattern to match unwanted characters\n",
        "unwanted_pattern = r\"[!@&\\\\.;:,/\\|()_{}\\\"\\'\\[\\]]\""
      ],
      "metadata": {
        "id": "nS8xZYWpZlL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counter = Counter()\n",
        "for text in merged_df:\n",
        "  # Use regular expressions to remove unwanted characters\n",
        "  text_words = re.sub(unwanted_pattern, \"\", text)\n",
        "  # Split the text into individual words\n",
        "  text_words = text_words.split()\n",
        "  # Exclude words that are too short\n",
        "  text_words = [w for w in text_words if len(w) > 3]\n",
        "  # Exclude unwanted words\n",
        "  text_words = [w for w in text_words if w not in [\"amp\"]]\n",
        "  word_counter.update(text_words)"
      ],
      "metadata": {
        "id": "kA1UBcaoZiAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "cloud = WordCloud(width=800, height=400)\n",
        "cloud.generate_from_frequencies(dict(word_counter.most_common(300)))\n",
        "image = cloud.to_image()\n",
        "image.save(\"wordcloud.png\")"
      ],
      "metadata": {
        "id": "CwldasLIZjCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load logo image\n",
        "logo_mask = np.array(Image.open(\"elephant.png\"))\n",
        "\n",
        "# Create WordCloud object\n",
        "cloud = WordCloud(width=1000, height=400, mask=logo_mask)\n",
        "\n",
        "# Generate wordcloud from word frequencies\n",
        "cloud.generate_from_frequencies(dict(word_counter.most_common(300)))\n",
        "\n",
        "# Convert to image\n",
        "image = cloud.to_image()\n",
        "\n",
        "# Save image\n",
        "image.save(\"wordcloud1.png\")"
      ],
      "metadata": {
        "id": "u141x-xsaNiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv('merged_df.csv')"
      ],
      "metadata": {
        "id": "694Rs7p6aEy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}